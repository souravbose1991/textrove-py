{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "professional-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import swifter\n",
    "# from bs4 import BeautifulSoup\n",
    "from itertools import chain\n",
    "import re, string, contractions, unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cross-democracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Users\\soura\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "critical-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.models import Phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import ldaseqmodel\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.matutils import hellinger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "allied-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-straight",
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATA_PATH, RESULTS_PATH, MODEL_PATH\n",
    "\n",
    "# DATA_PATH = \"../data\"\n",
    "# RESULTS_PATH = \"../results\"\n",
    "# MODEL_PATH = \"../model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Stopwords list ##################\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "custom_stp = ['say','million','year','quarter', 'month', 'thank','company','see', 'hi', 'hello', 'how',\n",
    "                 'think','get','look','okay','well','question', 'billion','management','be', 'sgd', 'usd', 'i', 'I', 'want', 'go', \n",
    "                 'thanks','morning','please','afternoon','mean','sort','guess','lot', 'right', 'much', 'would', 'could', 'bit', \n",
    "                 'like', 'give','take','expect','wonder','try','might','maybe','may','mr', 'sir', 'ahead', 'lady', 'gentleman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words + custom_stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class documents:   \n",
    "    def __init__(self, stop_words=None, num_topics=None, company=None, \n",
    "                 struct_df=None, subset_df=None, uniquetimes=None, time_slices=None, time_df = None,\n",
    "                 dictionary=None, corpus=None, texts=None, ldamodel=None):\n",
    "        self.stop_words = stop_words\n",
    "        self.struct_df = struct_df\n",
    "        self.subset_df = subset_df\n",
    "        self.uniquetimes = uniquetimes\n",
    "        self.time_slices = time_slices\n",
    "        self.time_df = time_df\n",
    "#         self.vis_obj = vis_obj\n",
    "        self.num_topics = num_topics\n",
    "        self.company = company\n",
    "        self.ldamodel = ldamodel\n",
    "        self.dictionary = dictionary\n",
    "        self.corpus = corpus\n",
    "        self.texts = texts\n",
    "        \n",
    "    \n",
    "    ################## Text Cleaning ##################\n",
    "        \n",
    "    def flatten(self, listOfLists):\n",
    "        \"Flatten one level of nesting\"\n",
    "        return list(chain.from_iterable(listOfLists))\n",
    "    \n",
    "    # Lemmatize with POS Tag\n",
    "    def __get_wordnet_pos(self, word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def __strip_html_tags(self, text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "        return stripped_text\n",
    "\n",
    "    def __remove_accented_chars(self, text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "    def __expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def __remove_special_characters(self, text, remove_digits=False):\n",
    "        pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        return text\n",
    "\n",
    "    def __pre_process_document(self, document):        \n",
    "        # converting to text\n",
    "        document =  str(document)\n",
    "\n",
    "        # strip HTML\n",
    "        document = self.__strip_html_tags(document)\n",
    "\n",
    "        # lower case\n",
    "        document = document.lower()\n",
    "\n",
    "        # remove extra newlines (often might be present in really noisy text)\n",
    "        document = document.translate(document.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        document = self.__remove_accented_chars(document)\n",
    "        document = re.sub(r\"x+\", \"\", document)\n",
    "        document = re.sub(r\"(<br/>)\", \"\", document)\n",
    "        document = re.sub(r\"(<a).*(>).*(</a>)\", \"\", document)\n",
    "        document = re.sub(r\"(&amp)\", \"\", document)\n",
    "        document = re.sub(r\"(&gt)\", \"\", document)\n",
    "        document = re.sub(r\"(&lt)\", \"\", document)\n",
    "        document = re.sub(r\"(\\xa0)\", \" \", document)\n",
    "\n",
    "        # remove special characters and\\or digits    \n",
    "        # insert spaces between special characters to isolate them    \n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        document = special_char_pattern.sub(\" \\\\1 \", document)\n",
    "        document = self.__remove_special_characters(document, remove_digits=True)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        document = re.sub(' +', ' ', document)\n",
    "        document = document.strip()\n",
    "        \n",
    "        # expand contractions    \n",
    "        document = self.__expand_contractions(document)\n",
    "        \n",
    "        # lemmatize\n",
    "        document = [token.lemma_ for token in nlp(document) if not token.is_stop]\n",
    "        document = [word for word in document if not word in self.stop_words]\n",
    "        document = \" \".join(document)\n",
    "\n",
    "        # Split the documents into tokens.\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        document = tokenizer.tokenize(document)  # Split into words.\n",
    "\n",
    "        # Remove words that are only one character.\n",
    "        document = [token for token in document if len(document) > 1]   \n",
    "        document = \" \".join(document)\n",
    "\n",
    "        return document\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    " ################## Topic Modeling Formatted output ##################\n",
    "\n",
    "    def format_topics_sentences(self, ldamodel=None, corpus=None, texts=None):\n",
    "        # Init output\n",
    "        sent_topics_df = pd.DataFrame()\n",
    "\n",
    "        # Get main topic in each document\n",
    "        for i, row_list in enumerate(ldamodel[corpus]):\n",
    "            row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "            # print(row)\n",
    "            row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "            # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # => dominant topic\n",
    "                    wp = ldamodel.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                    sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "        sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "        # Add original text to the end of the output\n",
    "        contents = pd.Series(texts)\n",
    "        sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "        return sent_topics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
